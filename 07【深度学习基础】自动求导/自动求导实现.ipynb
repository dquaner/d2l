{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e4cbc55-2a97-4a74-afb6-e3b2ea2040d7",
   "metadata": {},
   "source": [
    "# 自动求导\n",
    "\n",
    "深度学习框架通过自动计算导数，即自动微分（automatic differentiation）来加快求导。实际中，根据设计好的模型，系统会构建一个计算图（computational graph），来跟踪计算是那些数据通过哪些操作组合起来产生输出。自动微分使系统能够随后反向传播梯度。这里，反向传播（back propagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。\n",
    "\n",
    "## 1. 一个简单的例子\n",
    "\n",
    "假设我们想对函数 $y=2x^{\\top}x$ 关于列向量 $x$ 求导："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bd168ea-ba08-4d9d-b3aa-b30fd4d97e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(4.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bc9a15-cd98-4fc4-bea5-b00c433b8afe",
   "metadata": {},
   "source": [
    "在我们计算 y 关于 x 的梯度之前，我们需要一个地方来储存梯度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7af99a79-b0bc-4610-8944-8bb4dd684cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad_(True)  # 等价于 x=torch.arange(4.0, requires_grad=True)\n",
    "x.grad  # 默认是 None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e9da96-ba62-4247-9224-1cd4fd55c8ba",
   "metadata": {},
   "source": [
    "现在让我们计算 $y$："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46ddc276-df7d-42e6-8f6e-5a2aba06543e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 2 * torch.dot(x, x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9964100-91cf-4878-b5c7-bbbdfb85e50f",
   "metadata": {},
   "source": [
    "通过调用反向传播函数来自动计算 $y$ 关于 $x$ 每个分量的梯度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a0c2e66-5200-4b12-a804-1cedf2cc53c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d16ba52e-ce94-4294-9b3c-9a7d2c30ea05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3616d9-8f26-4410-a92a-4077dcffce80",
   "metadata": {},
   "source": [
    "现在让我们计算 $x$ 的另一个函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dd1b53f-7185-469b-8c0e-d786cfb75d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 默认情况下， PyTorch 会累积梯度，我们需要清楚之前的值\n",
    "x.grad.zero_()\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de9e39c-5167-4580-be87-fc8c265a95c4",
   "metadata": {},
   "source": [
    "## 2. 非标量变量的反向传播\n",
    "\n",
    "当 $y$ 不是标量时，向量 $y$ 关于向量 $x$ 的导数的最自然解释是一个矩阵。对于高阶和高维的 $y$ 和 $x$，求导的结果可以是一个高阶张量。\n",
    "\n",
    "深度学习中，当调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。我们的目的不是计算微分矩阵，而是批量中每个样本单独计算的偏导数之和："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34ae58b2-772a-4c71-bb8f-653a5d63cabf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对非标量调用 backward 需要传入一个 gradient 参数，该参数指定微分函数关于 self 的梯度\n",
    "# 本例只想求偏导数的和，所以传递一个 1 的梯度是合适的\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "# 等价于 y.backward(torch.ones(len(x)))\n",
    "y.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c918142-896f-4a56-b3aa-8015c1666102",
   "metadata": {},
   "source": [
    "### backward() 方法中 gradient 参数的意义：\n",
    "\n",
    "backward() 方法的作用是求出某个张量对于某些**标量**节点的梯度，当 $y$ 不是标量时，直接调用 backward() 方法会抛出 `Runtime Error: grad can be implicitly created only for scalar outputs`。\n",
    "\n",
    "因此，如果 $y$ 是矩阵（或向量），要先把 $y$ 转化为标量，再求导。转化的方法是：backward() 函数传入一个矩阵 $m$，计算 ${y} \\cdot {m}$（矩阵中的元素对应相乘再求和），就得到一个标量（实际上就是 $y$ 中的元素加权求和），然后才能求导。\n",
    "\n",
    "举个例子：\n",
    "\n",
    "设 $x = [x_1, x_2, x_3], z = x_1^2 + x_2^2 + x_3^2  + 6$，那么 `z.backword()` 表示的就是张量 $x$ 对于标量 $z$ 的梯度，即：$[\\frac{\\partial{z}}{\\partial{x_1}}, \\frac{\\partial{z}}{\\partial{x_2}}, \\frac{\\partial{z}}{\\partial{x_3}}]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3447f90-a73f-405f-98df-fa406caaf4dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.6000, -0.7555, -0.2983]], requires_grad=True),\n",
       " tensor([[-1.2000, -1.5111, -0.5966]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(size=((1, 3)), requires_grad=True)\n",
    "y = x ** 2 + 2\n",
    "z = torch.sum(y)\n",
    "z.backward()\n",
    "x, x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0711ea1a-40d2-4a0b-bd27-e933d4bd0d05",
   "metadata": {},
   "source": [
    "但是，当我们想对一个向量使用 backward() 的时候，就必须想办法让向量变为标量。\n",
    "\n",
    "举个例子：\n",
    "\n",
    "设\n",
    "\n",
    "$$\n",
    "x = [x_1, x_2, x_3], y = [y_1, y_2]\n",
    "$$\n",
    "$$\n",
    "y_1 = x_1x_2x_3\n",
    "$$\n",
    "$$\n",
    "y_2 = x_1 + x_2 + x_3\n",
    "$$\n",
    "\n",
    "在这里，我们使用一个函数 $a = g(y)$（先不关心函数的具体实现），来把变量 $y$ 变成一个标量，然后我们就可以对 $a$ 使用 backward()，表示张量 $x$ 对标量 $a$ 的梯度，即：$[\\frac{\\partial{a}}{\\partial{x_1}}, \\frac{\\partial{a}}{\\partial{x_2}}, \\frac{\\partial{a}}{\\partial{x_3}}]$ .\n",
    "\n",
    "从数学公式角度去理解：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{a}}{\\partial{x_1}} = \n",
    "\\frac{\\partial{a}}{\\partial{y_1}}\\frac{\\partial{y_1}}{\\partial{x_1}} + \n",
    "\\frac{\\partial{a}}{\\partial{y_2}}\\frac{\\partial{y_2}}{\\partial{x_1}}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial{a}}{\\partial{x_2}} = \n",
    "\\frac{\\partial{a}}{\\partial{y_1}}\\frac{\\partial{y_1}}{\\partial{x_2}} + \n",
    "\\frac{\\partial{a}}{\\partial{y_2}}\\frac{\\partial{y_2}}{\\partial{x_2}}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial{a}}{\\partial{x_3}} = \n",
    "\\frac{\\partial{a}}{\\partial{y_1}}\\frac{\\partial{y_1}}{\\partial{x_3}} + \n",
    "\\frac{\\partial{a}}{\\partial{y_2}}\\frac{\\partial{y_2}}{\\partial{x_3}}\n",
    "$$\n",
    "\n",
    "写成矩阵形式即：\n",
    "\n",
    "$$\n",
    "[\\frac{\\partial{a}}{\\partial{x_1}}, \\frac{\\partial{a}}{\\partial{x_2}}, \\frac{\\partial{a}}{\\partial{x_3}}] = \n",
    "[\\frac{\\partial{a}}{\\partial{y_1}}, \\frac{\\partial{a}}{\\partial{y_2}}]\n",
    "\\left[\n",
    "\\begin{array}{ccc}\n",
    "\\frac{\\partial{y_1}}{\\partial{x_1}} & \\frac{\\partial{y_1}}{\\partial{x_2}} & \\frac{\\partial{y_1}}{\\partial{x_3}} \\\\\n",
    "\\frac{\\partial{y_2}}{\\partial{x_1}} & \\frac{\\partial{y_2}}{\\partial{x_2}} & \\frac{\\partial{y_2}}{\\partial{x_3}} \n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "其中，\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}{ccc}\n",
    "\\frac{\\partial{y_1}}{\\partial{x_1}} & \\frac{\\partial{y_1}}{\\partial{x_2}} & \\frac{\\partial{y_1}}{\\partial{x_3}} \\\\\n",
    "\\frac{\\partial{y_2}}{\\partial{x_1}} & \\frac{\\partial{y_2}}{\\partial{x_2}} & \\frac{\\partial{y_2}}{\\partial{x_3}} \n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "是可以计算得出的，而由于 $a = g(y)$ 函数未确定，所以 $[\\frac{\\partial{a}}{\\partial{y_1}}, \\frac{\\partial{a}}{\\partial{y_2}}]$ 是无法计算的。\n",
    " \n",
    "其实 `gradient` 参数的作用就是定义出这个 $[\\frac{\\partial{a}}{\\partial{y_1}}, \\frac{\\partial{a}}{\\partial{y_2}}]$ 。\n",
    "\n",
    "回到代码中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d28c5e11-2879-4333-82a6-35bd5b455c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2.], requires_grad=True), tensor([0.4000, 0.2000, 0.2000]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(3.0, requires_grad=True)\n",
    "\n",
    "y = torch.randn(2) \n",
    "y[0] = x[0]*x[1]*x[2]\n",
    "y[1] = x[0]+x[1]+x[2]\n",
    "\n",
    "y.backward(torch.tensor([0.1, 0.2]))\n",
    "x, x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d664be17-18b5-48e9-95de-c81692a77847",
   "metadata": {},
   "source": [
    "## 3. 分离计算\n",
    "\n",
    "有时，我们希望将某些计算移动到记录的计算图之外。例如，假设 $y$ 是作为 $x$ 的函数计算的，而 $z$ 则是作为 $y$ 和 $x$ 的函数计算的。想象一下，我们想计算 $z$ 关于 $x$ 的梯度，但由于某种原因，希望将 $y$ 视为一个常数，并且只考虑到 $x$ 在 $y$ 被计算后发挥的作用。\n",
    "\n",
    "这里可以分离 $y$ 来返回一个新变量 $u$，该变量与 $y$ 具有相同的值，但丢弃计算图中如何计算 $y$ 的任何信息。换句话说，梯度不会向后流经 $u$ 到 $x$。因此，下面的反向传播函数计算 $z=u*x$ 关于 $x$ 的偏导数，同时将 $u$ 作为常数处理，而不是 $z=x*x*x$ 关于 $x$ 的偏导数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "989fecba-56d3-4618-b6c2-4716b19c79fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 4.]), tensor([True, True, True]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach()\n",
    "z = u * x\n",
    "\n",
    "z.sum().backward()\n",
    "x.grad, x.grad == u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89a0f96f-ae32-48fb-b471-08873cb63598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "x.grad == 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0cca18-1579-4cef-80ec-fb6bf9959170",
   "metadata": {},
   "source": [
    "## 4. Python 控制流的梯度计算\n",
    "\n",
    "即使构建函数的计算图需要通过 Python 控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到变量的梯度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85445931-752d-4687-956a-8c63b85eb356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c\n",
    "\n",
    "a = torch.randn(size=(), requires_grad=True)\n",
    "d = f(a)\n",
    "d.backward()\n",
    "\n",
    "a.grad == d / a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ea49de-0348-4df5-a363-cba9c027aec6",
   "metadata": {},
   "source": [
    "## 5. 小结\n",
    "\n",
    "深度学习框架可以自动计算导数：我们首先将梯度附加到想要对其计算偏导数的变量上，然后记录目标值的计算，执行它的反向传播函数，并访问得到的梯度。\n",
    "\n",
    "## 6. 练习\n",
    "\n",
    "1. 为什么计算二阶导数比一阶导数的开销要更大？"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5dd8424c-eff4-4e76-9dfa-365181a650e9",
   "metadata": {},
   "source": [
    "二阶导数是一阶导数的导数，需要在一阶导数上进行计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86cd308-ac85-48ae-b3b7-6d9f026f282a",
   "metadata": {},
   "source": [
    "2. 在运行反向传播函数后，立即再次运行它，看看会发生什么。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce3a024f-9d19-4e98-b313-f24586fab826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.arange(4.0, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d83a94c-e7fd-4a6a-8419-ba89a582505f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 2 * torch.dot(x, x)\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "671eaee9-a5d9-47aa-9c3b-54859d552fdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m x\u001b[38;5;241m.\u001b[39mgrad\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l-zh/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l-zh/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65d5713e-5d22-4125-a277-9aebdb1e7002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.arange(4.0, requires_grad=True)\n",
    "y = 2 * torch.dot(x, x)\n",
    "y.backward(retain_graph=True)\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8dd6febc-0d08-4a46-9d80-e33b82d387ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  8., 16., 24.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward(retain_graph=True)\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ae64245-208b-42df-ba06-3013b1388b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0., 12., 24., 36.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af586b22-e412-4e3e-969f-a480fadf2307",
   "metadata": {},
   "source": [
    "3. 在控制流的例子中，我们计算 d 关于 a 的导数，如果将变量 a 更改为随机向量或矩阵，会发生什么？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b20d4b81-baee-4ac5-9003-edaf8dfd3502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01c7f419-84a6-4c14-a3b8-dad013eb515a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: -0.013132723979651928\n",
      "d: -172133.234375\n",
      "a.grad: 13107200.0\n",
      "d / a: 13107200.0\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(size=(), requires_grad=True)\n",
    "d = f(a)\n",
    "d.backward()\n",
    "\n",
    "print(f'a: {a}')\n",
    "print(f'd: {d}')\n",
    "print(f'a.grad: {a.grad}')\n",
    "print(f'd / a: {d / a}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b010615-bab1-4f0c-8a3b-0d967405afe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.shape: torch.Size([6, 1])\n",
      "d.shape: torch.Size([6, 1])\n",
      "a.grad.shape: torch.Size([6, 1])\n",
      "d / a: tensor([[102400.0000],\n",
      "        [102400.0000],\n",
      "        [102400.0000],\n",
      "        [102400.0000],\n",
      "        [102400.0000],\n",
      "        [102400.0078]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(size=((6, 1)), requires_grad=True)\n",
    "print(f'a.shape: {a.shape}')\n",
    "\n",
    "d = f(a)\n",
    "print(f'd.shape: {d.shape}')\n",
    "\n",
    "# d.backward() -> grad can be implicitly created only for scalar outputs\n",
    "d.backward(torch.ones(len(a),1))  # 等价于 d.sum().backward()\n",
    "print(f'a.grad.shape: {a.grad.shape}')\n",
    "print(f'd / a: {d / a}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d444d12-a11a-486a-82db-c07666d151de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.shape: torch.Size([2, 3])\n",
      "d.shape: torch.Size([2, 3])\n",
      "a.grad.shape: torch.Size([2, 3])\n",
      "d / a: tensor([[1024., 1024., 1024.],\n",
      "        [1024., 1024., 1024.]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(size=((2,3)), requires_grad=True)\n",
    "print(f'a.shape: {a.shape}')\n",
    "\n",
    "d = f(a)\n",
    "print(f'd.shape: {d.shape}')\n",
    "\n",
    "# d.backward() -> grad can be implicitly created only for scalar outputs\n",
    "d.backward(torch.ones(d.shape))  # 等价于 d.sum().backward()\n",
    "print(f'a.grad.shape: {a.grad.shape}')\n",
    "print(f'd / a: {d / a}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52215db-5864-4b0b-8d6e-129654d9b2e1",
   "metadata": {},
   "source": [
    "4. 重新设计一个求控制流梯度的例子，运行并分析结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b513a291-3f51-4bd3-9e5f-9010caf8e75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([-0.0625,  1.0725, -1.9651, -1.1932, -0.2193, -0.0090,  0.1104, -0.4684,\n",
      "        -1.9621, -0.0152], requires_grad=True)\n",
      "d: tensor([ 33.1075, 130.8838,   4.0049,   7.0271,  26.7376,  35.5710,  41.6495,\n",
      "         18.8862,   4.0058,  35.2752], grad_fn=<PowBackward0>)\n",
      "count: 2\n",
      "a.grad: tensor([ 44.5928, 140.6044,   0.2797,   8.5545,  36.8306,  47.4988,  54.4782,\n",
      "         26.6244,   0.3038,  47.1524])\n",
      "d(x, count): tensor([ 44.5928, 140.6044,   0.2797,   8.5545,  36.8306,  47.4988,  54.4782,\n",
      "         26.6244,   0.3038,  47.1524], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True, True, True])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    count = 0\n",
    "    while x.norm() < 100:\n",
    "        x = (x + 2) ** 2\n",
    "        count += 1\n",
    "    return x, count\n",
    "\n",
    "def dx(x, count):\n",
    "    if count == 0:\n",
    "        return torch.ones(x.shape)\n",
    "    r = 2 ** count\n",
    "    m = x + 2\n",
    "    for i in range(count):\n",
    "        r = r * m\n",
    "        m = m ** 2 + 2\n",
    "    return r\n",
    "    \n",
    "\n",
    "a = torch.randn(size=(10,), requires_grad=True)\n",
    "print(f'a: {a}')\n",
    "\n",
    "d, count = f(a)\n",
    "print(f'd: {d}')\n",
    "print(f'count: {count}')\n",
    "\n",
    "# d.backward() -> grad can be implicitly created only for scalar outputs\n",
    "d.backward(torch.ones(d.shape))  # 等价于 d.sum().backward()\n",
    "print(f'a.grad: {a.grad}')\n",
    "print(f'd(x, count): {dx(a, count)}')\n",
    "a.grad == dx(a, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9b27a8-06f9-4e66-8301-e6e89dda8ad1",
   "metadata": {},
   "source": [
    "5. 使 $ f(x)=\\sin{(x)}$，绘制 $f(x)$ 和 $\\frac{df(x)}{dx}$ 的图像，其中后者不使用 $f'(x)=\\cos{(x)}$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ecccd2db-8844-47c9-a7ae-ceb50706b8c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"254.660937pt\" height=\"183.35625pt\" viewBox=\"0 0 254.660937 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2023-10-11T19:24:14.878999</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 183.35625 \n",
       "L 254.660937 183.35625 \n",
       "L 254.660937 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 52.160938 145.8 \n",
       "L 247.460938 145.8 \n",
       "L 247.460938 7.2 \n",
       "L 52.160938 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 61.03821 145.8 \n",
       "L 61.03821 7.2 \n",
       "\" clip-path=\"url(#p82f45c65b3)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_2\">\n",
       "      <defs>\n",
       "       <path id=\"m6e2aa2879a\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6e2aa2879a\" x=\"61.03821\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- −10 -->\n",
       "      <g transform=\"translate(50.485866 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"147.412109\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 105.446778 145.8 \n",
       "L 105.446778 7.2 \n",
       "\" clip-path=\"url(#p82f45c65b3)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6e2aa2879a\" x=\"105.446778\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- −5 -->\n",
       "      <g transform=\"translate(98.075684 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 149.855346 145.8 \n",
       "L 149.855346 7.2 \n",
       "\" clip-path=\"url(#p82f45c65b3)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6e2aa2879a\" x=\"149.855346\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(146.674096 160.398438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 194.263914 145.8 \n",
       "L 194.263914 7.2 \n",
       "\" clip-path=\"url(#p82f45c65b3)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6e2aa2879a\" x=\"194.263914\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(191.082664 160.398438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 238.672482 145.8 \n",
       "L 238.672482 7.2 \n",
       "\" clip-path=\"url(#p82f45c65b3)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6e2aa2879a\" x=\"238.672482\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(232.309982 160.398438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- x -->\n",
       "     <g transform=\"translate(146.851563 174.076563) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \n",
       "L 2247 1797 \n",
       "L 3578 0 \n",
       "L 2900 0 \n",
       "L 1881 1375 \n",
       "L 863 0 \n",
       "L 184 0 \n",
       "L 1544 1831 \n",
       "L 300 3500 \n",
       "L 978 3500 \n",
       "L 1906 2253 \n",
       "L 2834 3500 \n",
       "L 3513 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-78\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path d=\"M 52.160938 139.50002 \n",
       "L 247.460938 139.50002 \n",
       "\" clip-path=\"url(#p82f45c65b3)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_12\">\n",
       "      <defs>\n",
       "       <path id=\"mb78e10d889\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb78e10d889\" x=\"52.160938\" y=\"139.50002\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- −1.0 -->\n",
       "      <g transform=\"translate(20.878125 143.299239) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"179.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path d=\"M 52.160938 108.000015 \n",
       "L 247.460938 108.000015 \n",
       "\" clip-path=\"url(#p82f45c65b3)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb78e10d889\" x=\"52.160938\" y=\"108.000015\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- −0.5 -->\n",
       "      <g transform=\"translate(20.878125 111.799234) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <path d=\"M 52.160938 76.50001 \n",
       "L 247.460938 76.50001 \n",
       "\" clip-path=\"url(#p82f45c65b3)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb78e10d889\" x=\"52.160938\" y=\"76.50001\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(29.257812 80.299229) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <path d=\"M 52.160938 45.000005 \n",
       "L 247.460938 45.000005 \n",
       "\" clip-path=\"url(#p82f45c65b3)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_18\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb78e10d889\" x=\"52.160938\" y=\"45.000005\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.5 -->\n",
       "      <g transform=\"translate(29.257812 48.799224) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <path d=\"M 52.160938 13.5 \n",
       "L 247.460938 13.5 \n",
       "\" clip-path=\"url(#p82f45c65b3)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_20\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb78e10d889\" x=\"52.160938\" y=\"13.5\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 1.0 -->\n",
       "      <g transform=\"translate(29.257812 17.299219) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_12\">\n",
       "     <!-- f(x) -->\n",
       "     <g transform=\"translate(14.798438 85.121094) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \n",
       "L 2375 4384 \n",
       "L 1825 4384 \n",
       "Q 1516 4384 1395 4259 \n",
       "Q 1275 4134 1275 3809 \n",
       "L 1275 3500 \n",
       "L 2222 3500 \n",
       "L 2222 3053 \n",
       "L 1275 3053 \n",
       "L 1275 0 \n",
       "L 697 0 \n",
       "L 697 3053 \n",
       "L 147 3053 \n",
       "L 147 3500 \n",
       "L 697 3500 \n",
       "L 697 3744 \n",
       "Q 697 4328 969 4595 \n",
       "Q 1241 4863 1831 4863 \n",
       "L 2375 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \n",
       "Q 1566 4138 1362 3434 \n",
       "Q 1159 2731 1159 2009 \n",
       "Q 1159 1288 1364 580 \n",
       "Q 1569 -128 1984 -844 \n",
       "L 1484 -844 \n",
       "Q 1016 -109 783 600 \n",
       "Q 550 1309 550 2009 \n",
       "Q 550 2706 781 3412 \n",
       "Q 1013 4119 1484 4856 \n",
       "L 1984 4856 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-29\" d=\"M 513 4856 \n",
       "L 1013 4856 \n",
       "Q 1481 4119 1714 3412 \n",
       "Q 1947 2706 1947 2009 \n",
       "Q 1947 1309 1714 600 \n",
       "Q 1481 -109 1013 -844 \n",
       "L 513 -844 \n",
       "Q 928 -128 1133 580 \n",
       "Q 1338 1288 1338 2009 \n",
       "Q 1338 2731 1133 3434 \n",
       "Q 928 4138 513 4856 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-66\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-28\" x=\"35.205078\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-78\" x=\"74.21875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-29\" x=\"133.398438\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_21\">\n",
       "    <path d=\"M 61.03821 42.226675 \n",
       "L 62.90337 53.999134 \n",
       "L 65.567884 72.393933 \n",
       "L 69.919924 102.463479 \n",
       "L 71.873901 114.364338 \n",
       "L 73.472609 122.767034 \n",
       "L 74.804866 128.637386 \n",
       "L 75.959489 132.781979 \n",
       "L 76.936478 135.549417 \n",
       "L 77.824649 137.446592 \n",
       "L 78.624003 138.633806 \n",
       "L 79.245723 139.209828 \n",
       "L 79.867443 139.478697 \n",
       "L 80.400346 139.463649 \n",
       "L 80.933249 139.221999 \n",
       "L 81.554969 138.654882 \n",
       "L 82.176689 137.78333 \n",
       "L 82.887226 136.420037 \n",
       "L 83.775397 134.178185 \n",
       "L 84.752385 131.047555 \n",
       "L 85.818191 126.881848 \n",
       "L 87.061631 121.110803 \n",
       "L 88.571522 112.941721 \n",
       "L 90.347865 102.005559 \n",
       "L 92.745928 85.716145 \n",
       "L 98.25259 47.7894 \n",
       "L 100.11775 36.730205 \n",
       "L 101.627641 29.037136 \n",
       "L 102.959898 23.379174 \n",
       "L 104.114521 19.436703 \n",
       "L 105.09151 16.850733 \n",
       "L 105.979681 15.124881 \n",
       "L 106.690218 14.185109 \n",
       "L 107.311938 13.689644 \n",
       "L 107.844841 13.509769 \n",
       "L 108.377744 13.556592 \n",
       "L 108.910646 13.829942 \n",
       "L 109.532366 14.4338 \n",
       "L 110.242903 15.495928 \n",
       "L 111.042258 17.156818 \n",
       "L 111.930429 19.564915 \n",
       "L 112.907418 22.869777 \n",
       "L 114.06204 27.607698 \n",
       "L 115.394297 34.094015 \n",
       "L 116.904189 42.587703 \n",
       "L 118.769349 54.400663 \n",
       "L 121.52268 73.451538 \n",
       "L 125.697085 102.280507 \n",
       "L 127.651062 114.203761 \n",
       "L 129.249771 122.630598 \n",
       "L 130.582028 128.524473 \n",
       "L 131.73665 132.691524 \n",
       "L 132.713639 135.479174 \n",
       "L 133.60181 137.395466 \n",
       "L 134.401164 138.600328 \n",
       "L 135.022884 139.190271 \n",
       "L 135.644604 139.473157 \n",
       "L 136.177507 139.470147 \n",
       "L 136.71041 139.240513 \n",
       "L 137.33213 138.687326 \n",
       "L 137.95385 137.829546 \n",
       "L 138.664387 136.481711 \n",
       "L 139.552558 134.258615 \n",
       "L 140.529547 131.147682 \n",
       "L 141.595352 127.002074 \n",
       "L 142.838792 121.252273 \n",
       "L 144.259867 113.616136 \n",
       "L 146.036209 102.762875 \n",
       "L 148.345455 87.1585 \n",
       "L 154.296203 46.296196 \n",
       "L 156.072546 35.914289 \n",
       "L 157.582437 28.347279 \n",
       "L 158.825877 23.149595 \n",
       "L 159.9805 19.256091 \n",
       "L 160.957488 16.713969 \n",
       "L 161.756842 15.170474 \n",
       "L 162.467379 14.214939 \n",
       "L 163.089099 13.705522 \n",
       "L 163.622002 13.513623 \n",
       "L 164.154905 13.548407 \n",
       "L 164.687808 13.809749 \n",
       "L 165.309528 14.399692 \n",
       "L 166.020065 15.446125 \n",
       "L 166.819419 17.089745 \n",
       "L 167.70759 19.479295 \n",
       "L 168.684579 22.764751 \n",
       "L 169.839202 27.481391 \n",
       "L 171.171459 33.945823 \n",
       "L 172.68135 42.418755 \n",
       "L 174.54651 54.212853 \n",
       "L 177.299841 73.251114 \n",
       "L 181.563064 102.671638 \n",
       "L 183.517041 114.546834 \n",
       "L 185.115749 122.92192 \n",
       "L 186.448006 128.765396 \n",
       "L 187.602629 132.884348 \n",
       "L 188.579617 135.628723 \n",
       "L 189.467789 137.504092 \n",
       "L 190.267143 138.671183 \n",
       "L 190.888863 139.231336 \n",
       "L 191.421766 139.466978 \n",
       "L 191.954668 139.476007 \n",
       "L 192.487571 139.25839 \n",
       "L 193.109291 138.71914 \n",
       "L 193.731011 137.875139 \n",
       "L 194.441548 136.542776 \n",
       "L 195.240902 134.585306 \n",
       "L 196.217891 131.556239 \n",
       "L 197.283697 127.494275 \n",
       "L 198.527137 121.833033 \n",
       "L 199.948211 114.284098 \n",
       "L 201.635736 104.0834 \n",
       "L 203.856165 89.21279 \n",
       "L 210.428633 44.281722 \n",
       "L 212.204975 34.168272 \n",
       "L 213.62605 27.275398 \n",
       "L 214.86949 22.270396 \n",
       "L 215.935295 18.821835 \n",
       "L 216.912284 16.388408 \n",
       "L 217.711638 14.936832 \n",
       "L 218.422175 14.064614 \n",
       "L 219.043895 13.628906 \n",
       "L 219.576798 13.500499 \n",
       "L 220.109701 13.598823 \n",
       "L 220.642603 13.923523 \n",
       "L 221.264323 14.58685 \n",
       "L 221.97486 15.716008 \n",
       "L 222.774215 17.450603 \n",
       "L 223.662386 19.93778 \n",
       "L 224.728192 23.665805 \n",
       "L 225.882814 28.560075 \n",
       "L 227.215071 35.20667 \n",
       "L 228.81378 44.392054 \n",
       "L 230.767757 56.99487 \n",
       "L 233.965174 79.348028 \n",
       "L 237.340225 102.488967 \n",
       "L 238.583665 110.243025 \n",
       "L 238.583665 110.243025 \n",
       "\" clip-path=\"url(#p82f45c65b3)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_22\">\n",
       "    <path d=\"M 61.03821 129.361525 \n",
       "L 62.192833 133.358468 \n",
       "L 63.169821 135.993268 \n",
       "L 64.057993 137.765165 \n",
       "L 64.76853 138.742585 \n",
       "L 65.39025 139.27138 \n",
       "L 65.923153 139.479982 \n",
       "L 66.456056 139.461924 \n",
       "L 66.988958 139.217272 \n",
       "L 67.610678 138.646672 \n",
       "L 68.321215 137.622063 \n",
       "L 69.12057 136.002466 \n",
       "L 70.008741 133.638725 \n",
       "L 70.985729 130.380288 \n",
       "L 72.140352 125.693296 \n",
       "L 73.472609 119.25938 \n",
       "L 74.982501 110.81543 \n",
       "L 76.84766 99.047738 \n",
       "L 79.512174 80.656147 \n",
       "L 83.953031 50.009346 \n",
       "L 85.907008 38.173871 \n",
       "L 87.4169 30.267051 \n",
       "L 88.749157 24.390813 \n",
       "L 89.903779 20.240601 \n",
       "L 90.880768 17.468108 \n",
       "L 91.768939 15.566151 \n",
       "L 92.568293 14.374524 \n",
       "L 93.190013 13.795022 \n",
       "L 93.811733 13.522648 \n",
       "L 94.344636 13.534687 \n",
       "L 94.877539 13.773332 \n",
       "L 95.499259 14.336968 \n",
       "L 96.120979 15.205078 \n",
       "L 96.831516 16.564507 \n",
       "L 97.719687 18.801673 \n",
       "L 98.696676 21.927381 \n",
       "L 99.762481 26.088067 \n",
       "L 101.005921 31.853807 \n",
       "L 102.515813 40.017387 \n",
       "L 104.292155 50.948596 \n",
       "L 106.690218 67.234249 \n",
       "L 112.19688 105.165955 \n",
       "L 114.06204 116.230893 \n",
       "L 115.571932 123.929879 \n",
       "L 116.904189 129.593858 \n",
       "L 118.058811 133.542039 \n",
       "L 119.0358 136.133125 \n",
       "L 119.923971 137.863799 \n",
       "L 120.634508 138.807513 \n",
       "L 121.256228 139.306466 \n",
       "L 121.789131 139.489347 \n",
       "L 122.322034 139.445534 \n",
       "L 122.854937 139.175186 \n",
       "L 123.476657 138.574806 \n",
       "L 124.187194 137.516601 \n",
       "L 124.986548 135.860027 \n",
       "L 125.874719 133.456564 \n",
       "L 126.851708 130.15655 \n",
       "L 128.006331 125.423946 \n",
       "L 129.338588 118.943094 \n",
       "L 130.848479 110.454587 \n",
       "L 132.713639 98.646331 \n",
       "L 135.46697 79.598591 \n",
       "L 139.641375 50.765297 \n",
       "L 141.595352 38.836463 \n",
       "L 143.194061 30.403604 \n",
       "L 144.526318 24.503858 \n",
       "L 145.680941 20.331199 \n",
       "L 146.657929 17.538501 \n",
       "L 147.546101 15.617432 \n",
       "L 148.345455 14.40816 \n",
       "L 148.967175 13.814738 \n",
       "L 149.588895 13.528348 \n",
       "L 150.121797 13.528348 \n",
       "L 150.6547 13.754978 \n",
       "L 151.27642 14.304681 \n",
       "L 151.89814 15.159017 \n",
       "L 152.608677 16.502986 \n",
       "L 153.496849 18.721389 \n",
       "L 154.473837 21.827393 \n",
       "L 155.539643 25.967969 \n",
       "L 156.783083 31.71245 \n",
       "L 158.204157 39.343359 \n",
       "L 159.9805 50.191552 \n",
       "L 162.289745 65.792078 \n",
       "L 168.240493 106.659787 \n",
       "L 170.016836 117.047347 \n",
       "L 171.526727 124.620375 \n",
       "L 172.770167 129.823725 \n",
       "L 173.92479 133.722961 \n",
       "L 174.901778 136.270212 \n",
       "L 175.701133 137.81805 \n",
       "L 176.41167 138.777525 \n",
       "L 177.03339 139.290429 \n",
       "L 177.566292 139.485334 \n",
       "L 178.099195 139.453559 \n",
       "L 178.632098 139.19522 \n",
       "L 179.253818 138.608757 \n",
       "L 179.964355 137.566249 \n",
       "L 180.763709 135.926949 \n",
       "L 181.651881 133.542039 \n",
       "L 182.628869 130.26144 \n",
       "L 183.783492 125.550128 \n",
       "L 185.115749 119.091178 \n",
       "L 186.62564 110.623448 \n",
       "L 188.4908 98.834085 \n",
       "L 191.244131 79.799007 \n",
       "L 195.507354 50.374025 \n",
       "L 197.461331 38.493184 \n",
       "L 199.060039 30.112032 \n",
       "L 200.392296 24.262652 \n",
       "L 201.546919 20.138069 \n",
       "L 202.523908 17.388631 \n",
       "L 203.412079 15.508475 \n",
       "L 204.211433 14.336968 \n",
       "L 204.833153 13.773332 \n",
       "L 205.366056 13.534687 \n",
       "L 205.898959 13.522648 \n",
       "L 206.431862 13.73726 \n",
       "L 207.053582 14.273026 \n",
       "L 207.675302 15.113579 \n",
       "L 208.385839 16.442073 \n",
       "L 209.185193 18.395306 \n",
       "L 210.162181 21.419412 \n",
       "L 211.227987 25.476301 \n",
       "L 212.471427 31.132164 \n",
       "L 213.892501 38.67579 \n",
       "L 215.580027 48.871524 \n",
       "L 217.800455 63.738097 \n",
       "L 224.372923 108.675176 \n",
       "L 226.149266 118.794579 \n",
       "L 227.57034 125.693296 \n",
       "L 228.81378 130.704073 \n",
       "L 229.879585 134.157986 \n",
       "L 230.856574 136.596577 \n",
       "L 231.655928 138.052515 \n",
       "L 232.366465 138.928685 \n",
       "L 232.988185 139.367886 \n",
       "L 233.521088 139.499301 \n",
       "L 234.053991 139.403986 \n",
       "L 234.586894 139.082285 \n",
       "L 235.208614 138.422429 \n",
       "L 235.919151 137.297182 \n",
       "L 236.718505 135.566884 \n",
       "L 237.606676 133.084315 \n",
       "L 238.583665 129.701609 \n",
       "L 238.583665 129.701609 \n",
       "\" clip-path=\"url(#p82f45c65b3)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 52.160938 145.8 \n",
       "L 52.160938 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 247.460938 145.8 \n",
       "L 247.460938 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 52.160937 145.8 \n",
       "L 247.460938 145.8 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 52.160937 7.2 \n",
       "L 247.460938 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 188.470313 92.678125 \n",
       "L 240.460938 92.678125 \n",
       "Q 242.460938 92.678125 242.460938 90.678125 \n",
       "L 242.460938 62.321875 \n",
       "Q 242.460938 60.321875 240.460938 60.321875 \n",
       "L 188.470313 60.321875 \n",
       "Q 186.470313 60.321875 186.470313 62.321875 \n",
       "L 186.470313 90.678125 \n",
       "Q 186.470313 92.678125 188.470313 92.678125 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_23\">\n",
       "     <path d=\"M 190.470313 68.420313 \n",
       "L 200.470313 68.420313 \n",
       "L 210.470313 68.420313 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_13\">\n",
       "     <!-- f(x) -->\n",
       "     <g transform=\"translate(218.470313 71.920313) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-66\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-28\" x=\"35.205078\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-78\" x=\"74.21875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-29\" x=\"133.398438\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_24\">\n",
       "     <path d=\"M 190.470313 83.098438 \n",
       "L 200.470313 83.098438 \n",
       "L 210.470313 83.098438 \n",
       "\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n",
       "    </g>\n",
       "    <g id=\"text_14\">\n",
       "     <!-- f'(x) -->\n",
       "     <g transform=\"translate(218.470313 86.598438) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-27\" d=\"M 1147 4666 \n",
       "L 1147 2931 \n",
       "L 616 2931 \n",
       "L 616 4666 \n",
       "L 1147 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-66\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-27\" x=\"35.205078\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-28\" x=\"62.695312\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-78\" x=\"101.708984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-29\" x=\"160.888672\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p82f45c65b3\">\n",
       "   <rect x=\"52.160938\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib_inline import backend_inline\n",
    "from d2l import torch as d2l\n",
    "import numpy as np\n",
    "        \n",
    "def f(x):\n",
    "    return torch.sin(x)\n",
    "\n",
    "x = torch.tensor(np.arange(-10, 10, 0.01), requires_grad=True)\n",
    "y = f(x)\n",
    "y.backward(torch.ones(y.shape))\n",
    "d2l.plot(x.detach().numpy(), [f(x).detach().numpy(), x.grad.detach().numpy()], 'x', 'f(x)', legend=['f(x)', 'f\\'(x)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382f08db-1181-437c-b5e1-91eb787289af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
